    environment variable.
  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))
AlexNet(
  (features): DataParallel(
    (module): Sequential(
      (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))
      (1): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
      (4): BinConv2d(
        (bn): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
        (conv): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (relu): ReLU(inplace)
      )
      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
      (6): BinConv2d(
        (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
        (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): ReLU(inplace)
      )
      (7): BinConv2d(
        (bn): BatchNorm2d(384, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): ReLU(inplace)
      )
      (8): BinConv2d(
        (bn): BatchNorm2d(384, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
        (conv): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): ReLU(inplace)
      )
      (9): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (classifier): Sequential(
    (0): BinConv2d(
      (bn): BatchNorm1d(9216, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=9216, out_features=4096, bias=True)
      (relu): ReLU(inplace)
    )
    (1): BinConv2d(
      (dropout): Dropout(p=0.5)
      (bn): BatchNorm1d(4096, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)
      (linear): Linear(in_features=4096, out_features=4096, bias=True)
      (relu): ReLU(inplace)
    )
    (2): BatchNorm1d(4096, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    (3): Dropout(p=0.5)
    (4): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
Learning rate: 0.001
main.py:227: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  losses.update(loss.data[0], input.size(0))
Epoch: [0][0/2503]	Time 13.099 (13.099)	Data 1.088 (1.088)	Loss 6.9076 (6.9076)	Prec@1 0.000 (0.000)	Prec@5 0.781 (0.781)
Epoch: [0][100/2503]	Time 8.157 (8.239)	Data 0.719 (0.815)	Loss 6.8017 (6.8732)	Prec@1 0.781 (0.456)	Prec@5 1.953 (1.822)
Epoch: [0][200/2503]	Time 8.008 (8.197)	Data 0.734 (0.765)	Loss 6.5794 (6.7485)	Prec@1 0.586 (0.642)	Prec@5 3.516 (2.606)
Epoch: [0][300/2503]	Time 8.085 (8.199)	Data 0.747 (0.752)	Loss 6.2132 (6.6132)	Prec@1 2.148 (0.962)	Prec@5 5.469 (3.555)
Epoch: [0][400/2503]	Time 8.326 (8.202)	Data 0.772 (0.749)	Loss 5.8959 (6.4795)	Prec@1 2.930 (1.307)	Prec@5 10.742 (4.700)
Epoch: [0][500/2503]	Time 8.420 (8.204)	Data 0.782 (0.754)	Loss 5.7295 (6.3506)	Prec@1 4.297 (1.765)	Prec@5 10.352 (5.987)
Epoch: [0][600/2503]	Time 8.269 (8.206)	Data 0.801 (0.757)	Loss 5.6459 (6.2397)	Prec@1 4.883 (2.171)	Prec@5 15.234 (7.157)
Epoch: [0][700/2503]	Time 8.373 (8.215)	Data 0.807 (0.762)	Loss 5.4837 (6.1409)	Prec@1 4.102 (2.568)	Prec@5 16.992 (8.283)
Epoch: [0][800/2503]	Time 8.133 (8.218)	Data 0.811 (0.767)	Loss 5.2421 (6.0501)	Prec@1 5.664 (2.987)	Prec@5 19.531 (9.375)
Epoch: [0][900/2503]	Time 8.176 (8.227)	Data 0.827 (0.774)	Loss 5.2620 (5.9665)	Prec@1 7.422 (3.409)	Prec@5 21.875 (10.438)
Epoch: [0][1000/2503]	Time 8.246 (8.239)	Data 0.845 (0.780)	Loss 5.1165 (5.8916)	Prec@1 7.617 (3.823)	Prec@5 21.289 (11.433)
Epoch: [0][1100/2503]	Time 8.160 (8.246)	Data 0.857 (0.786)	Loss 5.0464 (5.8222)	Prec@1 10.547 (4.215)	Prec@5 21.289 (12.388)
Epoch: [0][1200/2503]	Time 8.390 (8.260)	Data 0.869 (0.793)	Loss 4.9782 (5.7572)	Prec@1 9.180 (4.613)	Prec@5 26.562 (13.299)
Epoch: [0][1300/2503]	Time 8.441 (8.271)	Data 0.906 (0.801)	Loss 4.9263 (5.6976)	Prec@1 9.766 (5.005)	Prec@5 27.344 (14.153)
Epoch: [0][1400/2503]	Time 8.488 (8.280)	Data 0.896 (0.807)	Loss 4.7351 (5.6408)	Prec@1 11.133 (5.380)	Prec@5 26.758 (15.004)
Epoch: [0][1500/2503]	Time 8.336 (8.288)	Data 0.912 (0.814)	Loss 4.8918 (5.5883)	Prec@1 8.594 (5.731)	Prec@5 24.609 (15.778)
Epoch: [0][1600/2503]	Time 8.348 (8.297)	Data 0.935 (0.821)	Loss 4.7318 (5.5377)	Prec@1 10.938 (6.089)	Prec@5 30.469 (16.562)
Epoch: [0][1700/2503]	Time 8.270 (8.305)	Data 0.936 (0.829)	Loss 4.6772 (5.4914)	Prec@1 11.914 (6.429)	Prec@5 27.930 (17.261)
Epoch: [0][1800/2503]	Time 8.390 (8.314)	Data 0.981 (0.836)	Loss 4.7362 (5.4464)	Prec@1 12.695 (6.771)	Prec@5 27.734 (17.958)
Epoch: [0][1900/2503]	Time 8.667 (8.327)	Data 0.985 (0.843)	Loss 4.6166 (5.4043)	Prec@1 12.891 (7.074)	Prec@5 30.859 (18.612)
Epoch: [0][2000/2503]	Time 8.423 (8.337)	Data 0.987 (0.850)	Loss 4.5525 (5.3637)	Prec@1 11.523 (7.389)	Prec@5 31.641 (19.251)
Epoch: [0][2100/2503]	Time 8.514 (8.348)	Data 1.010 (0.858)	Loss 4.3517 (5.3251)	Prec@1 15.430 (7.699)	Prec@5 37.109 (19.863)
Epoch: [0][2200/2503]	Time 8.482 (8.357)	Data 1.035 (0.865)	Loss 4.4441 (5.2892)	Prec@1 13.477 (7.998)	Prec@5 33.984 (20.448)
Epoch: [0][2300/2503]	Time 8.687 (8.367)	Data 1.018 (0.873)	Loss 4.2988 (5.2531)	Prec@1 17.578 (8.297)	Prec@5 36.719 (21.034)
Epoch: [0][2400/2503]	Time 8.707 (8.380)	Data 1.038 (0.880)	Loss 4.4725 (5.2187)	Prec@1 16.016 (8.588)	Prec@5 34.375 (21.593)
Epoch: [0][2500/2503]	Time 8.628 (8.391)	Data 1.098 (0.888)	Loss 4.4701 (5.1859)	Prec@1 13.086 (8.864)	Prec@5 32.617 (22.118)
main.py:270: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  input_var = torch.autograd.Variable(input, volatile=True)
main.py:271: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  target_var = torch.autograd.Variable(target, volatile=True)
main.py:279: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  losses.update(loss.data[0], input.size(0))
Test: [0/98]	Time 5.168 (5.168)	Loss 4.1746 (4.1746)	Prec@1 18.945 (18.945)	Prec@5 37.695 (37.695)
 * Prec@1 16.608 Prec@5 36.320
Learning rate: 0.001
Epoch: [1][0/2503]	Time 8.659 (8.659)	Data 1.278 (1.278)	Loss 4.4925 (4.4925)	Prec@1 16.016 (16.016)	Prec@5 33.594 (33.594)
Epoch: [1][100/2503]	Time 8.610 (8.585)	Data 1.074 (1.060)	Loss 4.3895 (4.3645)	Prec@1 15.234 (15.892)	Prec@5 35.352 (35.473)
Epoch: [1][200/2503]	Time 8.458 (8.583)	Data 1.079 (1.069)	Loss 4.2448 (4.3423)	Prec@1 17.188 (16.262)	Prec@5 35.938 (35.937)
Epoch: [1][300/2503]	Time 8.799 (8.604)	Data 1.094 (1.074)	Loss 4.2486 (4.3288)	Prec@1 17.383 (16.396)	Prec@5 36.133 (36.215)
Epoch: [1][400/2503]	Time 8.820 (8.630)	Data 1.132 (1.081)	Loss 4.1877 (4.3097)	Prec@1 20.898 (16.661)	Prec@5 39.453 (36.633)
Epoch: [1][500/2503]	Time 9.246 (8.640)	Data 1.136 (1.088)	Loss 4.1783 (4.2908)	Prec@1 18.164 (16.955)	Prec@5 40.039 (36.991)
Epoch: [1][600/2503]	Time 8.588 (8.654)	Data 1.142 (1.096)	Loss 4.1568 (4.2764)	Prec@1 18.945 (17.108)	Prec@5 38.477 (37.242)
Epoch: [1][700/2503]	Time 8.712 (8.671)	Data 1.136 (1.103)	Loss 4.0973 (4.2609)	Prec@1 19.336 (17.266)	Prec@5 41.602 (37.506)
Epoch: [1][800/2503]	Time 8.641 (8.689)	Data 1.176 (1.110)	Loss 4.0498 (4.2458)	Prec@1 19.141 (17.444)	Prec@5 41.406 (37.766)
Epoch: [1][900/2503]	Time 8.845 (8.703)	Data 1.184 (1.118)	Loss 4.0939 (4.2294)	Prec@1 18.164 (17.625)	Prec@5 40.234 (38.077)
Epoch: [1][1000/2503]	Time 9.005 (8.720)	Data 1.195 (1.126)	Loss 3.9363 (4.2161)	Prec@1 21.094 (17.799)	Prec@5 44.727 (38.337)
Epoch: [1][1100/2503]	Time 8.751 (8.735)	Data 1.215 (1.133)	Loss 4.0509 (4.2014)	Prec@1 18.945 (17.997)	Prec@5 41.211 (38.620)
Epoch: [1][1200/2503]	Time 8.859 (8.747)	Data 1.211 (1.141)	Loss 4.0790 (4.1874)	Prec@1 21.289 (18.178)	Prec@5 43.945 (38.892)
Epoch: [1][1300/2503]	Time 8.884 (8.759)	Data 1.248 (1.148)	Loss 3.9772 (4.1753)	Prec@1 21.289 (18.366)	Prec@5 42.383 (39.126)
Epoch: [1][1400/2503]	Time 8.865 (8.772)	Data 1.235 (1.156)	Loss 3.9264 (4.1623)	Prec@1 22.656 (18.547)	Prec@5 44.531 (39.363)
Epoch: [1][1500/2503]	Time 8.902 (8.784)	Data 1.291 (1.163)	Loss 3.8890 (4.1493)	Prec@1 20.508 (18.709)	Prec@5 45.312 (39.618)
Epoch: [1][1600/2503]	Time 9.045 (8.795)	Data 1.302 (1.170)	Loss 3.9513 (4.1366)	Prec@1 21.289 (18.888)	Prec@5 42.188 (39.875)
Epoch: [1][1700/2503]	Time 9.004 (8.806)	Data 1.310 (1.178)	Loss 3.8440 (4.1254)	Prec@1 21.484 (19.037)	Prec@5 45.703 (40.088)
Epoch: [1][1800/2503]	Time 9.065 (8.817)	Data 1.309 (1.185)	Loss 4.0143 (4.1129)	Prec@1 18.945 (19.199)	Prec@5 43.945 (40.328)
Epoch: [1][1900/2503]	Time 9.182 (8.828)	Data 1.316 (1.193)	Loss 3.8945 (4.1005)	Prec@1 21.094 (19.339)	Prec@5 45.117 (40.564)
Epoch: [1][2000/2503]	Time 8.949 (8.838)	Data 1.334 (1.200)	Loss 3.8177 (4.0889)	Prec@1 21.875 (19.488)	Prec@5 44.141 (40.783)
Epoch: [1][2100/2503]	Time 9.074 (8.850)	Data 1.362 (1.208)	Loss 3.6395 (4.0779)	Prec@1 26.953 (19.630)	Prec@5 47.852 (40.994)
Epoch: [1][2200/2503]	Time 8.968 (8.861)	Data 1.373 (1.215)	Loss 3.8278 (4.0673)	Prec@1 22.852 (19.767)	Prec@5 43.359 (41.201)
Epoch: [1][2300/2503]	Time 9.244 (8.874)	Data 1.378 (1.223)	Loss 3.7485 (4.0564)	Prec@1 24.023 (19.918)	Prec@5 47.656 (41.400)
Epoch: [1][2400/2503]	Time 9.362 (8.886)	Data 1.413 (1.230)	Loss 3.8297 (4.0456)	Prec@1 20.117 (20.067)	Prec@5 45.312 (41.605)
Epoch: [1][2500/2503]	Time 9.246 (8.896)	Data 1.416 (1.238)	Loss 3.7975 (4.0349)	Prec@1 19.922 (20.205)	Prec@5 46.680 (41.809)
Test: [0/98]	Time 5.264 (5.264)	Loss 3.6302 (3.6302)	Prec@1 25.586 (25.586)	Prec@5 50.195 (50.195)
 * Prec@1 24.674 Prec@5 47.954
Learning rate: 0.001
Epoch: [2][0/2503]	Time 8.973 (8.973)	Data 1.192 (1.192)	Loss 3.7790 (3.7790)	Prec@1 22.461 (22.461)	Prec@5 45.898 (45.898)
Epoch: [2][100/2503]	Time 8.963 (9.144)	Data 1.408 (1.403)	Loss 3.8581 (3.7659)	Prec@1 22.461 (23.501)	Prec@5 41.406 (46.726)
Epoch: [2][200/2503]	Time 9.028 (9.149)	Data 1.480 (1.416)	Loss 3.6537 (3.7541)	Prec@1 22.656 (23.872)	Prec@5 47.656 (47.112)
Epoch: [2][300/2503]	Time 9.114 (9.175)	Data 1.432 (1.425)	Loss 3.7821 (3.7519)	Prec@1 25.000 (23.938)	Prec@5 46.680 (47.101)
Epoch: [2][400/2503]	Time 9.613 (9.195)	Data 1.479 (1.432)	Loss 3.5269 (3.7399)	Prec@1 28.711 (24.095)	Prec@5 51.758 (47.331)
Epoch: [2][500/2503]	Time 9.367 (9.212)	Data 1.464 (1.440)	Loss 3.6600 (3.7306)	Prec@1 25.586 (24.230)	Prec@5 48.242 (47.532)
Epoch: [2][600/2503]	Time 9.168 (9.226)	Data 1.471 (1.449)	Loss 3.6298 (3.7225)	Prec@1 26.172 (24.369)	Prec@5 49.609 (47.646)
Epoch: [2][700/2503]	Time 9.214 (9.228)	Data 1.498 (1.456)	Loss 3.7306 (3.7155)	Prec@1 25.781 (24.477)	Prec@5 47.852 (47.776)
Epoch: [2][800/2503]	Time 9.090 (9.233)	Data 1.509 (1.463)	Loss 3.5590 (3.7074)	Prec@1 25.391 (24.611)	Prec@5 50.977 (47.917)
Epoch: [2][900/2503]	Time 9.171 (9.247)	Data 1.531 (1.472)	Loss 3.5560 (3.6987)	Prec@1 25.781 (24.719)	Prec@5 49.023 (48.070)
Epoch: [2][1000/2503]	Time 9.406 (9.259)	Data 1.550 (1.479)	Loss 3.5176 (3.6917)	Prec@1 29.883 (24.809)	Prec@5 49.219 (48.185)
Epoch: [2][1100/2503]	Time 9.229 (9.264)	Data 1.561 (1.486)	Loss 3.6311 (3.6845)	Prec@1 28.516 (24.921)	Prec@5 49.805 (48.319)
Epoch: [2][1200/2503]	Time 9.458 (9.274)	Data 1.647 (1.494)	Loss 3.6222 (3.6777)	Prec@1 26.367 (25.023)	Prec@5 50.781 (48.450)
Epoch: [2][1300/2503]	Time 9.445 (9.287)	Data 1.586 (1.502)	Loss 3.6706 (3.6712)	Prec@1 24.023 (25.119)	Prec@5 50.000 (48.574)
Epoch: [2][1400/2503]	Time 9.243 (9.301)	Data 1.602 (1.509)	Loss 3.5531 (3.6664)	Prec@1 25.586 (25.207)	Prec@5 51.367 (48.674)
Epoch: [2][1500/2503]	Time 9.433 (9.309)	Data 1.636 (1.517)	Loss 3.5731 (3.6601)	Prec@1 27.148 (25.284)	Prec@5 52.539 (48.798)
Epoch: [2][1600/2503]	Time 9.363 (9.317)	Data 1.632 (1.524)	Loss 3.5140 (3.6535)	Prec@1 26.953 (25.383)	Prec@5 53.125 (48.925)
Epoch: [2][1700/2503]	Time 9.398 (9.326)	Data 1.644 (1.531)	Loss 3.5510 (3.6481)	Prec@1 25.586 (25.460)	Prec@5 52.734 (49.020)
Epoch: [2][1800/2503]	Time 9.474 (9.334)	Data 1.687 (1.539)	Loss 3.5694 (3.6413)	Prec@1 24.023 (25.559)	Prec@5 48.633 (49.143)
Epoch: [2][1900/2503]	Time 9.364 (9.344)	Data 1.676 (1.546)	Loss 3.4985 (3.6351)	Prec@1 30.664 (25.637)	Prec@5 51.562 (49.261)
Epoch: [2][2000/2503]	Time 9.739 (9.356)	Data 1.704 (1.554)	Loss 3.4779 (3.6295)	Prec@1 29.102 (25.705)	Prec@5 52.344 (49.360)
Epoch: [2][2100/2503]	Time 9.546 (9.365)	Data 1.731 (1.561)	Loss 3.3079 (3.6244)	Prec@1 31.641 (25.782)	Prec@5 55.273 (49.454)
Epoch: [2][2200/2503]	Time 9.487 (9.374)	Data 1.786 (1.568)	Loss 3.4051 (3.6188)	Prec@1 26.758 (25.868)	Prec@5 52.930 (49.552)
Epoch: [2][2300/2503]	Time 9.597 (9.383)	Data 1.740 (1.576)	Loss 3.3962 (3.6133)	Prec@1 29.102 (25.953)	Prec@5 55.273 (49.657)
Epoch: [2][2400/2503]	Time 9.497 (9.393)	Data 1.751 (1.583)	Loss 3.4675 (3.6078)	Prec@1 27.148 (26.027)	Prec@5 51.172 (49.759)
Epoch: [2][2500/2503]	Time 9.533 (9.404)	Data 1.771 (1.591)	Loss 3.5279 (3.6020)	Prec@1 28.320 (26.113)	Prec@5 49.219 (49.867)
Test: [0/98]	Time 5.219 (5.219)	Loss 3.3316 (3.3316)	Prec@1 29.883 (29.883)	Prec@5 57.031 (57.031)
 * Prec@1 28.830 Prec@5 53.096
Learning rate: 0.001
Epoch: [3][0/2503]	Time 9.706 (9.706)	Data 1.668 (1.668)	Loss 3.4130 (3.4130)	Prec@1 26.758 (26.758)	Prec@5 53.906 (53.906)
Epoch: [3][100/2503]	Time 9.427 (9.621)	Data 1.765 (1.764)	Loss 3.4900 (3.4653)	Prec@1 26.562 (27.905)	Prec@5 49.609 (52.201)
Epoch: [3][200/2503]	Time 10.052 (9.623)	Data 1.809 (1.770)	Loss 3.3777 (3.4595)	Prec@1 27.539 (28.033)	Prec@5 54.297 (52.392)
Epoch: [3][300/2503]	Time 9.568 (9.629)	Data 1.776 (1.780)	Loss 3.4546 (3.4563)	Prec@1 28.320 (28.072)	Prec@5 53.125 (52.491)
Epoch: [3][400/2503]	Time 9.754 (9.631)	Data 1.829 (1.786)	Loss 3.3884 (3.4469)	Prec@1 31.250 (28.258)	Prec@5 55.664 (52.730)
Epoch: [3][500/2503]	Time 9.695 (9.635)	Data 1.865 (1.793)	Loss 3.3601 (3.4425)	Prec@1 31.055 (28.385)	Prec@5 55.078 (52.822)
Epoch: [3][600/2503]	Time 9.568 (9.640)	Data 1.854 (1.801)	Loss 3.3331 (3.4347)	Prec@1 32.031 (28.521)	Prec@5 54.297 (52.960)
Epoch: [3][700/2503]	Time 9.960 (9.652)	Data 1.850 (1.808)	Loss 3.4796 (3.4301)	Prec@1 25.977 (28.554)	Prec@5 51.562 (53.040)
Epoch: [3][800/2503]	Time 9.612 (9.664)	Data 1.862 (1.816)	Loss 3.2226 (3.4245)	Prec@1 29.688 (28.640)	Prec@5 58.203 (53.168)
Epoch: [3][900/2503]	Time 9.791 (9.681)	Data 1.881 (1.823)	Loss 3.2895 (3.4174)	Prec@1 29.492 (28.738)	Prec@5 55.859 (53.301)
Epoch: [3][1000/2503]	Time 9.842 (9.690)	Data 1.886 (1.831)	Loss 3.2414 (3.4136)	Prec@1 32.422 (28.790)	Prec@5 56.250 (53.372)
Epoch: [3][1100/2503]	Time 9.705 (9.701)	Data 1.906 (1.837)	Loss 3.4326 (3.4081)	Prec@1 29.102 (28.873)	Prec@5 54.297 (53.475)
Epoch: [3][1200/2503]	Time 9.681 (9.710)	Data 1.922 (1.845)	Loss 3.2843 (3.4025)	Prec@1 29.883 (28.963)	Prec@5 55.664 (53.576)
Epoch: [3][1300/2503]	Time 9.834 (9.717)	Data 1.932 (1.852)	Loss 3.2823 (3.3984)	Prec@1 27.930 (29.041)	Prec@5 53.711 (53.653)
Epoch: [3][1400/2503]	Time 9.714 (9.726)	Data 1.942 (1.858)	Loss 3.3485 (3.3944)	Prec@1 29.492 (29.111)	Prec@5 55.273 (53.725)
Epoch: [3][1500/2503]	Time 10.005 (9.736)	Data 1.998 (1.866)	Loss 3.3178 (3.3899)	Prec@1 30.469 (29.186)	Prec@5 55.859 (53.809)
Epoch: [3][1600/2503]	Time 9.947 (9.749)	Data 1.979 (1.873)	Loss 3.1939 (3.3854)	Prec@1 30.078 (29.260)	Prec@5 56.250 (53.902)
Epoch: [3][1700/2503]	Time 10.196 (9.763)	Data 2.012 (1.881)	Loss 3.3079 (3.3830)	Prec@1 32.227 (29.299)	Prec@5 56.445 (53.945)
Epoch: [3][1800/2503]	Time 9.851 (9.776)	Data 2.037 (1.888)	Loss 3.3181 (3.3778)	Prec@1 30.664 (29.381)	Prec@5 55.469 (54.034)
Epoch: [3][1900/2503]	Time 10.081 (9.786)	Data 2.031 (1.896)	Loss 3.3630 (3.3740)	Prec@1 28.906 (29.432)	Prec@5 55.859 (54.099)
Epoch: [3][2000/2503]	Time 10.006 (9.799)	Data 2.050 (1.903)	Loss 3.2465 (3.3706)	Prec@1 29.688 (29.485)	Prec@5 57.812 (54.166)
Epoch: [3][2100/2503]	Time 10.023 (9.809)	Data 2.072 (1.911)	Loss 3.1399 (3.3671)	Prec@1 35.742 (29.546)	Prec@5 57.617 (54.224)
Epoch: [3][2200/2503]	Time 9.812 (9.818)	Data 2.069 (1.918)	Loss 3.2078 (3.3633)	Prec@1 33.203 (29.616)	Prec@5 56.836 (54.287)
Epoch: [3][2300/2503]	Time 9.837 (9.826)	Data 2.092 (1.926)	Loss 3.0505 (3.3595)	Prec@1 37.109 (29.680)	Prec@5 59.766 (54.359)
Epoch: [3][2400/2503]	Time 9.894 (9.834)	Data 2.114 (1.933)	Loss 3.4039 (3.3556)	Prec@1 29.688 (29.731)	Prec@5 53.711 (54.426)
Epoch: [3][2500/2503]	Time 10.052 (9.843)	Data 2.143 (1.941)	Loss 3.4087 (3.3515)	Prec@1 27.539 (29.790)	Prec@5 51.172 (54.495)
Test: [0/98]	Time 5.386 (5.386)	Loss 3.1616 (3.1616)	Prec@1 33.594 (33.594)	Prec@5 59.375 (59.375)
 * Prec@1 31.580 Prec@5 56.548
Learning rate: 0.001
Epoch: [4][0/2503]	Time 9.186 (9.186)	Data 1.279 (1.279)	Loss 3.2194 (3.2194)	Prec@1 28.516 (28.516)	Prec@5 58.984 (58.984)
Epoch: [4][100/2503]	Time 10.035 (10.031)	Data 2.111 (2.108)	Loss 3.3261 (3.2496)	Prec@1 29.102 (31.153)	Prec@5 52.344 (56.293)
Epoch: [4][200/2503]	Time 9.945 (10.006)	Data 2.144 (2.116)	Loss 3.1345 (3.2423)	Prec@1 30.859 (31.259)	Prec@5 57.422 (56.480)
Epoch: [4][300/2503]	Time 9.890 (10.025)	Data 2.150 (2.126)	Loss 3.3778 (3.2437)	Prec@1 30.664 (31.364)	Prec@5 53.906 (56.409)
Epoch: [4][400/2503]	Time 10.271 (10.039)	Data 2.171 (2.133)	Loss 3.0456 (3.2372)	Prec@1 34.961 (31.553)	Prec@5 62.695 (56.546)
Epoch: [4][500/2503]	Time 10.058 (10.045)	Data 2.169 (2.140)	Loss 3.1848 (3.2361)	Prec@1 32.812 (31.611)	Prec@5 59.766 (56.593)
Epoch: [4][600/2503]	Time 10.127 (10.057)	Data 2.177 (2.147)	Loss 3.2599 (3.2311)	Prec@1 30.664 (31.662)	Prec@5 56.445 (56.682)
Epoch: [4][700/2503]	Time 10.489 (10.071)	Data 2.191 (2.155)	Loss 3.1851 (3.2289)	Prec@1 32.812 (31.682)	Prec@5 57.617 (56.695)
Epoch: [4][800/2503]	Time 10.071 (10.079)	Data 2.228 (2.163)	Loss 3.0027 (3.2258)	Prec@1 33.594 (31.735)	Prec@5 58.203 (56.750)
Epoch: [4][900/2503]	Time 10.074 (10.085)	Data 2.237 (2.172)	Loss 3.1205 (3.2205)	Prec@1 30.859 (31.775)	Prec@5 56.641 (56.859)
Epoch: [4][1000/2503]	Time 10.297 (10.095)	Data 2.247 (2.178)	Loss 3.0304 (3.2169)	Prec@1 33.789 (31.832)	Prec@5 59.375 (56.928)
Epoch: [4][1100/2503]	Time 10.317 (10.106)	Data 2.258 (2.186)	Loss 3.4122 (3.2132)	Prec@1 31.250 (31.890)	Prec@5 52.539 (56.984)
Epoch: [4][1200/2503]	Time 10.164 (10.115)	Data 2.284 (2.194)	Loss 3.1831 (3.2088)	Prec@1 32.227 (31.974)	Prec@5 57.422 (57.076)
Epoch: [4][1300/2503]	Time 10.431 (10.126)	Data 2.320 (2.201)	Loss 3.1830 (3.2073)	Prec@1 33.984 (31.998)	Prec@5 56.445 (57.107)
Epoch: [4][1400/2503]	Time 10.360 (10.135)	Data 2.292 (2.208)	Loss 3.0585 (3.2048)	Prec@1 35.742 (32.040)	Prec@5 62.500 (57.135)
Epoch: [4][1500/2503]	Time 10.521 (10.144)	Data 2.325 (2.215)	Loss 3.2320 (3.2022)	Prec@1 34.180 (32.090)	Prec@5 58.398 (57.179)
Epoch: [4][1600/2503]	Time 10.078 (10.156)	Data 2.338 (2.223)	Loss 3.0740 (3.1986)	Prec@1 32.812 (32.140)	Prec@5 59.570 (57.256)
Epoch: [4][1700/2503]	Time 10.322 (10.167)	Data 2.368 (2.230)	Loss 3.1135 (3.1969)	Prec@1 32.031 (32.162)	Prec@5 58.398 (57.286)
Epoch: [4][1800/2503]	Time 10.470 (10.176)	Data 2.401 (2.238)	Loss 3.1487 (3.1935)	Prec@1 31.836 (32.218)	Prec@5 59.961 (57.351)
Epoch: [4][1900/2503]	Time 10.383 (10.184)	Data 2.352 (2.245)	Loss 3.0131 (3.1912)	Prec@1 33.008 (32.253)	Prec@5 58.398 (57.394)
Epoch: [4][2000/2503]	Time 10.471 (10.193)	Data 2.387 (2.252)	Loss 3.0372 (3.1881)	Prec@1 32.227 (32.303)	Prec@5 59.180 (57.445)
Epoch: [4][2100/2503]	Time 10.512 (10.202)	Data 2.398 (2.259)	Loss 2.9493 (3.1852)	Prec@1 37.109 (32.350)	Prec@5 60.938 (57.494)
Epoch: [4][2200/2503]	Time 10.223 (10.210)	Data 2.412 (2.266)	Loss 2.9927 (3.1829)	Prec@1 33.008 (32.401)	Prec@5 59.766 (57.529)
Epoch: [4][2300/2503]	Time 10.450 (10.218)	Data 2.488 (2.273)	Loss 3.0551 (3.1799)	Prec@1 36.328 (32.442)	Prec@5 61.133 (57.586)
Epoch: [4][2400/2503]	Time 10.538 (10.227)	Data 2.431 (2.280)	Loss 3.1219 (3.1772)	Prec@1 31.836 (32.492)	Prec@5 58.008 (57.633)
Epoch: [4][2500/2503]	Time 10.306 (10.236)	Data 2.493 (2.288)	Loss 3.2308 (3.1745)	Prec@1 33.594 (32.532)	Prec@5 55.664 (57.684)
Test: [0/98]	Time 5.355 (5.355)	Loss 3.0161 (3.0161)	Prec@1 35.742 (35.742)	Prec@5 61.914 (61.914)
 * Prec@1 33.504 Prec@5 58.606
Learning rate: 0.001
Epoch: [5][0/2503]	Time 9.239 (9.239)	Data 1.295 (1.295)	Loss 3.1513 (3.1513)	Prec@1 31.055 (31.055)	Prec@5 58.398 (58.398)
Epoch: [5][100/2503]	Time 10.265 (10.384)	Data 2.470 (2.449)	Loss 3.2268 (3.1073)	Prec@1 31.250 (33.377)	Prec@5 54.688 (58.919)
Epoch: [5][200/2503]	Time 10.319 (10.439)	Data 2.503 (2.462)	Loss 2.9560 (3.0993)	Prec@1 35.547 (33.731)	Prec@5 61.719 (59.116)
Epoch: [5][300/2503]	Time 10.349 (10.444)	Data 2.510 (2.475)	Loss 3.1856 (3.0980)	Prec@1 33.984 (33.810)	Prec@5 59.180 (59.156)
Epoch: [5][400/2503]	Time 10.528 (10.449)	Data 2.544 (2.484)	Loss 2.9345 (3.0946)	Prec@1 35.938 (33.905)	Prec@5 63.867 (59.199)
Epoch: [5][500/2503]	Time 10.600 (10.459)	Data 2.509 (2.490)	Loss 3.0270 (3.0910)	Prec@1 37.305 (33.949)	Prec@5 62.305 (59.278)
Epoch: [5][600/2503]	Time 10.523 (10.464)	Data 2.528 (2.498)	Loss 3.0695 (3.0868)	Prec@1 31.641 (34.004)	Prec@5 60.547 (59.318)
Epoch: [5][700/2503]	Time 10.931 (10.472)	Data 2.536 (2.506)	Loss 3.1796 (3.0846)	Prec@1 32.617 (34.017)	Prec@5 58.398 (59.352)
Epoch: [5][800/2503]	Time 10.372 (10.479)	Data 2.586 (2.514)	Loss 2.7882 (3.0828)	Prec@1 38.086 (34.049)	Prec@5 66.406 (59.398)
Epoch: [5][900/2503]	Time 10.582 (10.491)	Data 2.595 (2.522)	Loss 3.0265 (3.0781)	Prec@1 33.398 (34.119)	Prec@5 60.547 (59.481)
Epoch: [5][1000/2503]	Time 10.611 (10.495)	Data 2.601 (2.528)	Loss 2.8977 (3.0745)	Prec@1 38.867 (34.159)	Prec@5 59.375 (59.548)
Epoch: [5][1100/2503]	Time 10.387 (10.503)	Data 2.620 (2.536)	Loss 3.2616 (3.0722)	Prec@1 32.227 (34.201)	Prec@5 56.445 (59.576)
Epoch: [5][1200/2503]	Time 10.557 (10.512)	Data 2.621 (2.544)	Loss 2.9522 (3.0690)	Prec@1 36.328 (34.266)	Prec@5 61.914 (59.619)
Epoch: [5][1300/2503]	Time 10.788 (10.520)	Data 2.631 (2.551)	Loss 3.0432 (3.0676)	Prec@1 33.984 (34.295)	Prec@5 58.398 (59.646)
Epoch: [5][1400/2503]	Time 10.944 (10.528)	Data 2.631 (2.559)	Loss 3.0072 (3.0661)	Prec@1 35.352 (34.311)	Prec@5 63.281 (59.679)
Epoch: [5][1500/2503]	Time 10.893 (10.538)	Data 2.665 (2.566)	Loss 3.0380 (3.0638)	Prec@1 34.766 (34.344)	Prec@5 60.742 (59.717)
Epoch: [5][1600/2503]	Time 10.418 (10.542)	Data 2.700 (2.573)	Loss 2.9908 (3.0611)	Prec@1 34.375 (34.388)	Prec@5 63.281 (59.771)
Epoch: [5][1700/2503]	Time 10.744 (10.550)	Data 2.710 (2.581)	Loss 2.9862 (3.0604)	Prec@1 35.156 (34.399)	Prec@5 59.961 (59.782)
Epoch: [5][1800/2503]	Time 10.657 (10.558)	Data 2.727 (2.588)	Loss 3.0221 (3.0577)	Prec@1 36.523 (34.431)	Prec@5 60.547 (59.840)
Epoch: [5][1900/2503]	Time 10.862 (10.566)	Data 2.758 (2.595)	Loss 2.9905 (3.0554)	Prec@1 34.766 (34.469)	Prec@5 59.180 (59.871)
Epoch: [5][2000/2503]	Time 10.648 (10.574)	Data 2.755 (2.603)	Loss 2.8814 (3.0532)	Prec@1 33.789 (34.502)	Prec@5 62.695 (59.906)
Epoch: [5][2100/2503]	Time 10.775 (10.583)	Data 2.748 (2.610)	Loss 2.8465 (3.0516)	Prec@1 40.820 (34.523)	Prec@5 61.914 (59.934)
Epoch: [5][2200/2503]	Time 10.722 (10.591)	Data 2.795 (2.618)	Loss 2.9503 (3.0498)	Prec@1 37.109 (34.546)	Prec@5 61.719 (59.974)
Epoch: [5][2300/2503]	Time 10.895 (10.598)	Data 2.783 (2.625)	Loss 2.9126 (3.0472)	Prec@1 37.305 (34.581)	Prec@5 63.086 (60.021)
Epoch: [5][2400/2503]	Time 10.746 (10.604)	Data 2.786 (2.632)	Loss 3.0397 (3.0452)	Prec@1 33.203 (34.612)	Prec@5 58.789 (60.056)
Epoch: [5][2500/2503]	Time 10.837 (10.611)	Data 2.815 (2.640)	Loss 3.1626 (3.0434)	Prec@1 30.859 (34.635)	Prec@5 58.594 (60.092)
Test: [0/98]	Time 5.420 (5.420)	Loss 2.9191 (2.9191)	Prec@1 39.062 (39.062)	Prec@5 61.133 (61.133)
 * Prec@1 35.478 Prec@5 60.724
Learning rate: 0.001
Epoch: [6][0/2503]	Time 9.792 (9.792)	Data 1.685 (1.685)	Loss 3.0452 (3.0452)	Prec@1 33.984 (33.984)	Prec@5 58.984 (58.984)
Epoch: [6][100/2503]	Time 10.902 (10.751)	Data 2.840 (2.800)	Loss 3.0047 (3.0056)	Prec@1 36.719 (35.127)	Prec@5 59.180 (60.961)
Epoch: [6][200/2503]	Time 10.715 (10.766)	Data 2.831 (2.814)	Loss 2.8711 (2.9890)	Prec@1 35.352 (35.513)	Prec@5 63.477 (61.120)
Epoch: [6][300/2503]	Time 10.697 (10.782)	Data 2.848 (2.822)	Loss 3.0608 (2.9897)	Prec@1 36.914 (35.524)	Prec@5 60.156 (61.038)
Epoch: [6][400/2503]	Time 10.665 (10.795)	Data 2.934 (2.831)	Loss 2.7768 (2.9818)	Prec@1 40.820 (35.678)	Prec@5 66.016 (61.181)
Epoch: [6][500/2503]	Time 11.199 (10.810)	Data 2.876 (2.839)	Loss 2.8904 (2.9803)	Prec@1 37.109 (35.718)	Prec@5 62.695 (61.233)
Epoch: [6][600/2503]	Time 10.966 (10.822)	Data 2.891 (2.847)	Loss 2.9878 (2.9772)	Prec@1 33.984 (35.737)	Prec@5 61.328 (61.258)
Epoch: [6][700/2503]	Time 10.749 (10.830)	Data 2.869 (2.855)	Loss 3.0070 (2.9762)	Prec@1 35.742 (35.732)	Prec@5 63.867 (61.300)
Epoch: [6][800/2503]	Time 10.968 (10.837)	Data 2.915 (2.862)	Loss 2.9539 (2.9729)	Prec@1 36.523 (35.799)	Prec@5 60.547 (61.338)
Epoch: [6][900/2503]	Time 10.906 (10.843)	Data 2.929 (2.869)	Loss 2.9952 (2.9686)	Prec@1 35.547 (35.856)	Prec@5 60.156 (61.399)
Epoch: [6][1000/2503]	Time 11.100 (10.852)	Data 2.948 (2.878)	Loss 2.8312 (2.9660)	Prec@1 39.062 (35.894)	Prec@5 63.867 (61.452)
Epoch: [6][1100/2503]	Time 10.729 (10.862)	Data 2.958 (2.885)	Loss 3.0978 (2.9635)	Prec@1 35.742 (35.935)	Prec@5 58.203 (61.499)
Epoch: [6][1200/2503]	Time 10.909 (10.869)	Data 2.974 (2.892)	Loss 2.8971 (2.9602)	Prec@1 39.258 (35.980)	Prec@5 63.281 (61.566)
Epoch: [6][1300/2503]	Time 11.014 (10.877)	Data 2.993 (2.899)	Loss 2.8939 (2.9586)	Prec@1 35.742 (36.011)	Prec@5 61.914 (61.574)
Epoch: [6][1400/2503]	Time 10.954 (10.886)	Data 3.009 (2.906)	Loss 2.9273 (2.9580)	Prec@1 35.156 (36.022)	Prec@5 63.477 (61.585)
Epoch: [6][1500/2503]	Time 11.386 (10.894)	Data 3.017 (2.914)	Loss 2.9440 (2.9563)	Prec@1 38.281 (36.057)	Prec@5 63.086 (61.622)
Epoch: [6][1600/2503]	Time 11.172 (10.903)	Data 3.045 (2.921)	Loss 2.9898 (2.9551)	Prec@1 33.594 (36.065)	Prec@5 61.328 (61.660)
Epoch: [6][1700/2503]	Time 10.874 (10.913)	Data 3.035 (2.928)	Loss 2.9947 (2.9547)	Prec@1 35.156 (36.070)	Prec@5 61.914 (61.666)
Epoch: [6][1800/2503]	Time 11.215 (10.922)	Data 3.054 (2.935)	Loss 2.9207 (2.9521)	Prec@1 38.281 (36.113)	Prec@5 62.109 (61.716)
Epoch: [6][1900/2503]	Time 11.239 (10.932)	Data 3.086 (2.943)	Loss 2.9524 (2.9505)	Prec@1 35.156 (36.139)	Prec@5 62.109 (61.741)
Epoch: [6][2000/2503]	Time 11.273 (10.941)	Data 3.094 (2.950)	Loss 2.8431 (2.9493)	Prec@1 36.914 (36.158)	Prec@5 61.914 (61.757)
Epoch: [6][2100/2503]	Time 11.288 (10.949)	Data 3.098 (2.958)	Loss 2.7692 (2.9481)	Prec@1 39.453 (36.178)	Prec@5 63.086 (61.777)
Epoch: [6][2200/2503]	Time 10.964 (10.957)	Data 3.088 (2.965)	Loss 2.8659 (2.9468)	Prec@1 36.914 (36.196)	Prec@5 64.062 (61.790)
Epoch: [6][2300/2503]	Time 11.171 (10.965)	Data 3.263 (2.972)	Loss 2.8510 (2.9450)	Prec@1 37.500 (36.215)	Prec@5 63.672 (61.821)
Epoch: [6][2400/2503]	Time 11.203 (10.973)	Data 3.153 (2.980)	Loss 3.0205 (2.9435)	Prec@1 33.398 (36.235)	Prec@5 59.961 (61.846)
Epoch: [6][2500/2503]	Time 11.094 (10.980)	Data 3.174 (2.987)	Loss 2.9139 (2.9417)	Prec@1 36.914 (36.262)	Prec@5 60.742 (61.873)
Test: [0/98]	Time 5.483 (5.483)	Loss 2.7922 (2.7922)	Prec@1 40.820 (40.820)	Prec@5 64.258 (64.258)
 * Prec@1 37.072 Prec@5 62.460
Learning rate: 0.001
Epoch: [7][0/2503]	Time 9.389 (9.389)	Data 1.385 (1.385)	Loss 2.8739 (2.8739)	Prec@1 34.375 (34.375)	Prec@5 64.844 (64.844)
Epoch: [7][100/2503]	Time 11.246 (11.161)	Data 3.161 (3.135)	Loss 2.9876 (2.9003)	Prec@1 34.570 (36.864)	Prec@5 60.938 (62.585)
Epoch: [7][200/2503]	Time 11.344 (11.166)	Data 3.169 (3.153)	Loss 2.7837 (2.8943)	Prec@1 35.352 (36.994)	Prec@5 64.258 (62.608)
Epoch: [7][300/2503]	Time 10.994 (11.174)	Data 3.195 (3.165)	Loss 3.0022 (2.8984)	Prec@1 36.914 (36.897)	Prec@5 61.914 (62.596)
Epoch: [7][400/2503]	Time 11.349 (11.178)	Data 3.237 (3.177)	Loss 2.6479 (2.8944)	Prec@1 39.648 (36.988)	Prec@5 68.359 (62.676)
Epoch: [7][500/2503]	Time 11.694 (11.232)	Data 3.265 (3.198)	Loss 2.8458 (2.8922)	Prec@1 37.305 (37.037)	Prec@5 63.867 (62.719)
Epoch: [7][600/2503]	Time 11.610 (11.308)	Data 3.253 (3.209)	Loss 2.8484 (2.8883)	Prec@1 37.891 (37.105)	Prec@5 65.234 (62.735)
Epoch: [7][700/2503]	Time 11.714 (11.362)	Data 3.298 (3.220)	Loss 2.9633 (2.8874)	Prec@1 36.523 (37.117)	Prec@5 61.328 (62.740)
Epoch: [7][800/2503]	Time 11.589 (11.403)	Data 3.301 (3.229)	Loss 2.6891 (2.8859)	Prec@1 38.672 (37.140)	Prec@5 65.625 (62.775)
Epoch: [7][900/2503]	Time 11.722 (11.430)	Data 3.311 (3.239)	Loss 2.7896 (2.8825)	Prec@1 38.281 (37.176)	Prec@5 64.258 (62.857)
Epoch: [7][1000/2503]	Time 11.540 (11.455)	Data 3.334 (3.249)	Loss 2.7040 (2.8799)	Prec@1 42.773 (37.230)	Prec@5 66.016 (62.931)
Epoch: [7][1100/2503]	Time 11.822 (11.482)	Data 3.350 (3.258)	Loss 2.9582 (2.8776)	Prec@1 34.180 (37.297)	Prec@5 61.523 (62.983)
Epoch: [7][1200/2503]	Time 11.822 (11.512)	Data 3.370 (3.268)	Loss 2.8066 (2.8755)	Prec@1 40.430 (37.341)	Prec@5 63.281 (63.034)
Epoch: [7][1300/2503]	Time 11.950 (11.532)	Data 3.386 (3.276)	Loss 2.9011 (2.8742)	Prec@1 37.695 (37.373)	Prec@5 63.281 (63.038)
Epoch: [7][1400/2503]	Time 11.948 (11.556)	Data 3.412 (3.284)	Loss 2.7594 (2.8728)	Prec@1 42.188 (37.399)	Prec@5 67.188 (63.061)
Epoch: [7][1500/2503]	Time 11.886 (11.581)	Data 3.430 (3.292)	Loss 2.8638 (2.8715)	Prec@1 37.891 (37.429)	Prec@5 64.258 (63.093)
Epoch: [7][1600/2503]	Time 11.993 (11.601)	Data 3.452 (3.301)	Loss 2.7244 (2.8704)	Prec@1 38.477 (37.452)	Prec@5 67.578 (63.123)
Epoch: [7][1700/2503]	Time 12.049 (11.616)	Data 3.450 (3.309)	Loss 2.8886 (2.8699)	Prec@1 36.523 (37.457)	Prec@5 63.477 (63.130)
Epoch: [7][1800/2503]	Time 12.086 (11.633)	Data 3.479 (3.317)	Loss 2.8749 (2.8679)	Prec@1 38.086 (37.497)	Prec@5 61.133 (63.159)
Epoch: [7][1900/2503]	Time 11.896 (11.651)	Data 3.482 (3.324)	Loss 2.7954 (2.8668)	Prec@1 37.500 (37.514)	Prec@5 61.914 (63.177)
Epoch: [7][2000/2503]	Time 12.214 (11.665)	Data 3.567 (3.332)	Loss 2.8267 (2.8655)	Prec@1 32.617 (37.540)	Prec@5 63.867 (63.193)
Epoch: [7][2100/2503]	Time 11.947 (11.682)	Data 3.495 (3.340)	Loss 2.7441 (2.8652)	Prec@1 38.086 (37.547)	Prec@5 65.234 (63.206)
Epoch: [7][2200/2503]	Time 11.853 (11.697)	Data 3.521 (3.348)	Loss 2.7831 (2.8644)	Prec@1 37.695 (37.572)	Prec@5 65.625 (63.216)
Epoch: [7][2300/2503]	Time 12.050 (11.709)	Data 3.578 (3.356)	Loss 2.6847 (2.8637)	Prec@1 38.867 (37.584)	Prec@5 67.969 (63.224)
Epoch: [7][2400/2503]	Time 11.922 (11.720)	Data 3.545 (3.363)	Loss 2.9935 (2.8626)	Prec@1 36.133 (37.612)	Prec@5 61.719 (63.246)
Epoch: [7][2500/2503]	Time 11.983 (11.728)	Data 3.572 (3.371)	Loss 2.9158 (2.8614)	Prec@1 35.547 (37.624)	Prec@5 65.234 (63.268)
Test: [0/98]	Time 5.620 (5.620)	Loss 2.7363 (2.7363)	Prec@1 41.211 (41.211)	Prec@5 65.820 (65.820)
 * Prec@1 37.796 Prec@5 63.286
Learning rate: 0.001
Epoch: [8][0/2503]	Time 9.657 (9.657)	Data 1.313 (1.313)	Loss 2.7693 (2.7693)	Prec@1 38.477 (38.477)	Prec@5 66.406 (66.406)
Epoch: [8][100/2503]	Time 11.885 (11.928)	Data 3.563 (3.534)	Loss 2.9373 (2.8258)	Prec@1 34.766 (37.916)	Prec@5 60.547 (63.979)
Epoch: [8][200/2503]	Time 11.990 (11.966)	Data 3.573 (3.551)	Loss 2.7278 (2.8244)	Prec@1 38.477 (38.127)	Prec@5 63.867 (63.888)
Epoch: [8][300/2503]	Time 11.932 (11.992)	Data 3.592 (3.562)	Loss 2.9157 (2.8318)	Prec@1 39.453 (38.076)	Prec@5 62.500 (63.769)
Epoch: [8][400/2503]	Time 12.138 (11.991)	Data 3.622 (3.569)	Loss 2.5651 (2.8261)	Prec@1 42.969 (38.188)	Prec@5 69.336 (63.910)
Epoch: [8][500/2503]	Time 12.047 (12.007)	Data 3.630 (3.575)	Loss 2.7759 (2.8231)	Prec@1 41.406 (38.233)	Prec@5 65.234 (63.949)
Epoch: [8][600/2503]	Time 12.186 (12.020)	Data 3.625 (3.581)	Loss 2.8411 (2.8181)	Prec@1 37.305 (38.324)	Prec@5 65.039 (64.001)
Epoch: [8][700/2503]	Time 12.184 (12.031)	Data 3.612 (3.589)	Loss 2.8447 (2.8170)	Prec@1 38.672 (38.321)	Prec@5 63.281 (64.037)
Epoch: [8][800/2503]	Time 12.086 (12.047)	Data 3.639 (3.597)	Loss 2.6727 (2.8151)	Prec@1 39.648 (38.397)	Prec@5 65.039 (64.064)
Epoch: [8][900/2503]	Time 12.165 (12.061)	Data 3.684 (3.605)	Loss 2.7949 (2.8112)	Prec@1 38.086 (38.452)	Prec@5 66.406 (64.159)
Epoch: [8][1000/2503]	Time 12.127 (12.070)	Data 3.684 (3.613)	Loss 2.6181 (2.8086)	Prec@1 42.383 (38.453)	Prec@5 68.164 (64.205)
Epoch: [8][1100/2503]	Time 12.060 (12.080)	Data 3.702 (3.622)	Loss 2.9478 (2.8077)	Prec@1 37.109 (38.476)	Prec@5 63.477 (64.229)
Epoch: [8][1200/2503]	Time 12.176 (12.088)	Data 3.741 (3.630)	Loss 2.7553 (2.8066)	Prec@1 39.062 (38.500)	Prec@5 66.016 (64.251)
Epoch: [8][1300/2503]	Time 12.341 (12.095)	Data 3.754 (3.638)	Loss 2.8876 (2.8051)	Prec@1 35.742 (38.542)	Prec@5 63.477 (64.275)
Epoch: [8][1400/2503]	Time 12.394 (12.102)	Data 3.748 (3.645)	Loss 2.7782 (2.8050)	Prec@1 41.211 (38.551)	Prec@5 62.695 (64.276)
Epoch: [8][1500/2503]	Time 12.379 (12.113)	Data 3.750 (3.654)	Loss 2.9372 (2.8038)	Prec@1 36.914 (38.571)	Prec@5 60.938 (64.295)
Epoch: [8][1600/2503]	Time 12.372 (12.118)	Data 3.762 (3.661)	Loss 2.8688 (2.8029)	Prec@1 37.305 (38.593)	Prec@5 64.062 (64.311)
Epoch: [8][1700/2503]	Time 12.139 (12.128)	Data 3.788 (3.668)	Loss 2.8192 (2.8030)	Prec@1 38.867 (38.600)	Prec@5 66.992 (64.312)
Epoch: [8][1800/2503]	Time 12.067 (12.137)	Data 3.779 (3.675)	Loss 2.8012 (2.8015)	Prec@1 37.891 (38.631)	Prec@5 64.453 (64.356)
Epoch: [8][1900/2503]	Time 12.194 (12.145)	Data 3.824 (3.683)	Loss 2.6967 (2.8008)	Prec@1 38.672 (38.637)	Prec@5 66.602 (64.368)
Epoch: [8][2000/2503]	Time 12.306 (12.152)	Data 3.849 (3.690)	Loss 2.7510 (2.7998)	Prec@1 37.305 (38.650)	Prec@5 65.430 (64.392)
Epoch: [8][2100/2503]	Time 12.293 (12.158)	Data 3.873 (3.698)	Loss 2.6540 (2.7991)	Prec@1 41.602 (38.658)	Prec@5 65.234 (64.390)
Epoch: [8][2200/2503]	Time 12.296 (12.166)	Data 3.872 (3.705)	Loss 2.6877 (2.7986)	Prec@1 38.672 (38.667)	Prec@5 67.188 (64.390)
Epoch: [8][2300/2503]	Time 12.321 (12.174)	Data 3.893 (3.713)	Loss 2.6541 (2.7984)	Prec@1 41.211 (38.674)	Prec@5 65.039 (64.391)
Epoch: [8][2400/2503]	Time 12.150 (12.183)	Data 3.880 (3.720)	Loss 2.8719 (2.7970)	Prec@1 35.547 (38.705)	Prec@5 63.672 (64.409)
Epoch: [8][2500/2503]	Time 12.339 (12.188)	Data 3.908 (3.727)	Loss 2.8664 (2.7958)	Prec@1 39.844 (38.721)	Prec@5 62.695 (64.436)
Test: [0/98]	Time 5.613 (5.613)	Loss 2.6965 (2.6965)	Prec@1 39.844 (39.844)	Prec@5 66.992 (66.992)
 * Prec@1 39.034 Prec@5 64.484
Learning rate: 0.001
Epoch: [9][0/2503]	Time 9.837 (9.837)	Data 1.435 (1.435)	Loss 2.7846 (2.7846)	Prec@1 37.500 (37.500)	Prec@5 63.477 (63.477)
Epoch: [9][100/2503]	Time 12.283 (12.309)	Data 3.917 (3.883)	Loss 2.8320 (2.7721)	Prec@1 35.938 (38.813)	Prec@5 61.523 (64.797)
Epoch: [9][200/2503]	Time 12.311 (12.350)	Data 3.934 (3.903)	Loss 2.6250 (2.7637)	Prec@1 39.062 (38.970)	Prec@5 65.625 (64.903)
Epoch: [9][300/2503]	Time 12.341 (12.371)	Data 3.913 (3.914)	Loss 2.8663 (2.7706)	Prec@1 35.547 (38.944)	Prec@5 65.234 (64.804)
Epoch: [9][400/2503]	Time 12.285 (12.372)	Data 3.959 (3.922)	Loss 2.5550 (2.7627)	Prec@1 42.188 (39.132)	Prec@5 67.773 (64.954)
Epoch: [9][500/2503]	Time 12.356 (12.390)	Data 3.970 (3.929)	Loss 2.6395 (2.7612)	Prec@1 40.039 (39.197)	Prec@5 67.773 (65.000)
Epoch: [9][600/2503]	Time 12.461 (12.392)	Data 3.964 (3.935)	Loss 2.8422 (2.7571)	Prec@1 36.719 (39.271)	Prec@5 64.453 (65.086)
Epoch: [9][700/2503]	Time 12.377 (12.397)	Data 3.972 (3.942)	Loss 2.7792 (2.7560)	Prec@1 40.430 (39.279)	Prec@5 67.773 (65.127)
Epoch: [9][800/2503]	Time 12.560 (12.411)	Data 4.032 (3.951)	Loss 2.6664 (2.7544)	Prec@1 40.430 (39.319)	Prec@5 67.188 (65.155)
Epoch: [9][900/2503]	Time 12.465 (12.423)	Data 4.018 (3.961)	Loss 2.7371 (2.7515)	Prec@1 39.844 (39.360)	Prec@5 66.406 (65.207)
Epoch: [9][1000/2503]	Time 12.578 (12.432)	Data 4.088 (3.969)	Loss 2.6370 (2.7497)	Prec@1 43.750 (39.398)	Prec@5 67.383 (65.245)
Epoch: [9][1100/2503]	Time 12.652 (12.437)	Data 4.043 (3.976)	Loss 2.8918 (2.7489)	Prec@1 38.281 (39.426)	Prec@5 63.477 (65.286)
Epoch: [9][1200/2503]	Time 12.433 (12.439)	Data 4.059 (3.983)	Loss 2.7015 (2.7467)	Prec@1 38.281 (39.473)	Prec@5 66.797 (65.313)
Epoch: [9][1300/2503]	Time 12.356 (12.440)	Data 4.059 (3.991)	Loss 2.8392 (2.7471)	Prec@1 37.305 (39.489)	Prec@5 62.109 (65.295)
Epoch: [9][1400/2503]	Time 12.603 (12.450)	Data 4.114 (3.998)	Loss 2.7429 (2.7469)	Prec@1 38.867 (39.494)	Prec@5 65.625 (65.297)
Epoch: [9][1500/2503]	Time 12.552 (12.461)	Data 4.127 (4.006)	Loss 2.7937 (2.7463)	Prec@1 39.062 (39.489)	Prec@5 64.648 (65.310)
Epoch: [9][1600/2503]	Time 12.666 (12.469)	Data 4.133 (4.013)	Loss 2.6528 (2.7452)	Prec@1 40.430 (39.510)	Prec@5 66.016 (65.335)
Epoch: [9][1700/2503]	Time 12.584 (12.477)	Data 4.150 (4.021)	Loss 2.7561 (2.7460)	Prec@1 37.305 (39.496)	Prec@5 64.648 (65.315)
Epoch: [9][1800/2503]	Time 12.511 (12.485)	Data 4.169 (4.028)	Loss 2.7389 (2.7445)	Prec@1 38.477 (39.529)	Prec@5 65.039 (65.340)
Epoch: [9][1900/2503]	Time 12.852 (12.494)	Data 4.161 (4.036)	Loss 2.6672 (2.7435)	Prec@1 40.625 (39.540)	Prec@5 66.016 (65.353)
Epoch: [9][2000/2503]	Time 12.425 (12.501)	Data 4.200 (4.043)	Loss 2.6493 (2.7425)	Prec@1 41.602 (39.558)	Prec@5 65.820 (65.368)
Epoch: [9][2100/2503]	Time 12.632 (12.505)	Data 4.199 (4.050)	Loss 2.6686 (2.7419)	Prec@1 41.797 (39.566)	Prec@5 68.164 (65.369)
Epoch: [9][2200/2503]	Time 12.525 (12.515)	Data 4.190 (4.057)	Loss 2.6946 (2.7411)	Prec@1 37.891 (39.579)	Prec@5 67.969 (65.384)
Epoch: [9][2300/2503]	Time 12.519 (12.523)	Data 4.213 (4.065)	Loss 2.5851 (2.7407)	Prec@1 43.555 (39.588)	Prec@5 67.969 (65.388)
Epoch: [9][2400/2503]	Time 12.849 (12.532)	Data 4.247 (4.072)	Loss 2.7348 (2.7397)	Prec@1 36.914 (39.603)	Prec@5 64.844 (65.399)
Epoch: [9][2500/2503]	Time 12.743 (12.537)	Data 4.256 (4.079)	Loss 2.8058 (2.7386)	Prec@1 37.305 (39.627)	Prec@5 64.062 (65.415)
Test: [0/98]	Time 5.650 (5.650)	Loss 2.7183 (2.7183)	Prec@1 41.211 (41.211)	Prec@5 64.453 (64.453)
 * Prec@1 39.742 Prec@5 64.982
Learning rate: 0.001
Epoch: [10][0/2503]	Time 9.999 (9.999)	Data 1.693 (1.693)	Loss 2.8054 (2.8054)	Prec@1 39.648 (39.648)	Prec@5 63.867 (63.867)
Epoch: [10][100/2503]	Time 12.525 (12.664)	Data 4.281 (4.223)	Loss 2.7808 (2.7286)	Prec@1 34.961 (39.623)	Prec@5 62.695 (65.509)
Epoch: [10][200/2503]	Time 12.703 (12.675)	Data 4.265 (4.245)	Loss 2.6962 (2.7185)	Prec@1 39.258 (39.853)	Prec@5 65.625 (65.770)
Epoch: [10][300/2503]	Time 12.715 (12.662)	Data 4.287 (4.253)	Loss 2.7202 (2.7196)	Prec@1 41.797 (39.857)	Prec@5 66.797 (65.783)
Epoch: [10][400/2503]	Time 12.763 (12.668)	Data 4.339 (4.263)	Loss 2.5353 (2.7151)	Prec@1 43.359 (39.950)	Prec@5 70.312 (65.852)
Epoch: [10][500/2503]	Time 12.785 (12.673)	Data 4.297 (4.272)	Loss 2.6452 (2.7151)	Prec@1 42.773 (40.019)	Prec@5 67.383 (65.850)
Epoch: [10][600/2503]	Time 12.692 (12.684)	Data 4.349 (4.280)	Loss 2.7203 (2.7118)	Prec@1 40.039 (40.063)	Prec@5 66.797 (65.897)
Epoch: [10][700/2503]	Time 12.912 (12.702)	Data 4.341 (4.289)	Loss 2.7875 (2.7116)	Prec@1 39.453 (40.083)	Prec@5 64.844 (65.910)
Epoch: [10][800/2503]	Time 12.750 (12.710)	Data 4.348 (4.296)	Loss 2.5835 (2.7110)	Prec@1 42.969 (40.123)	Prec@5 68.555 (65.921)
Epoch: [10][900/2503]	Time 12.935 (12.720)	Data 4.395 (4.304)	Loss 2.6444 (2.7076)	Prec@1 42.383 (40.181)	Prec@5 66.992 (65.968)
Epoch: [10][1000/2503]	Time 12.764 (12.739)	Data 4.392 (4.312)	Loss 2.6029 (2.7056)	Prec@1 43.750 (40.193)	Prec@5 66.406 (66.004)
Epoch: [10][1100/2503]	Time 13.103 (12.755)	Data 4.423 (4.320)	Loss 2.8524 (2.7041)	Prec@1 39.453 (40.225)	Prec@5 61.133 (66.024)
Epoch: [10][1200/2503]	Time 12.672 (12.765)	Data 4.398 (4.328)	Loss 2.6357 (2.7021)	Prec@1 41.211 (40.275)	Prec@5 67.188 (66.055)
Epoch: [10][1300/2503]	Time 12.892 (12.770)	Data 4.431 (4.335)	Loss 2.8059 (2.7015)	Prec@1 36.523 (40.301)	Prec@5 66.992 (66.073)
Epoch: [10][1400/2503]	Time 12.735 (12.775)	Data 4.440 (4.342)	Loss 2.7085 (2.7010)	Prec@1 41.406 (40.288)	Prec@5 66.992 (66.080)
Epoch: [10][1500/2503]	Time 12.931 (12.782)	Data 4.457 (4.349)	Loss 2.7553 (2.7004)	Prec@1 41.602 (40.296)	Prec@5 64.844 (66.098)
Epoch: [10][1600/2503]	Time 13.064 (12.791)	Data 4.521 (4.357)	Loss 2.7559 (2.7001)	Prec@1 36.914 (40.297)	Prec@5 66.406 (66.094)
Epoch: [10][1700/2503]	Time 12.887 (12.796)	Data 4.482 (4.364)	Loss 2.5676 (2.7005)	Prec@1 41.992 (40.280)	Prec@5 68.164 (66.096)
Epoch: [10][1800/2503]	Time 12.980 (12.798)	Data 4.478 (4.371)	Loss 2.5992 (2.6991)	Prec@1 42.969 (40.298)	Prec@5 67.969 (66.116)
Epoch: [10][1900/2503]	Time 12.963 (12.805)	Data 4.543 (4.379)	Loss 2.7000 (2.6984)	Prec@1 39.844 (40.310)	Prec@5 65.039 (66.119)
Epoch: [10][2000/2503]	Time 12.665 (12.812)	Data 4.549 (4.386)	Loss 2.6903 (2.6978)	Prec@1 37.500 (40.318)	Prec@5 65.039 (66.123)
Epoch: [10][2100/2503]	Time 12.915 (12.821)	Data 4.539 (4.394)	Loss 2.4541 (2.6976)	Prec@1 47.070 (40.320)	Prec@5 68.555 (66.130)
Epoch: [10][2200/2503]	Time 12.927 (12.826)	Data 4.529 (4.401)	Loss 2.5314 (2.6971)	Prec@1 44.141 (40.339)	Prec@5 68.555 (66.141)
Epoch: [10][2300/2503]	Time 12.934 (12.830)	Data 4.568 (4.408)	Loss 2.5484 (2.6964)	Prec@1 43.164 (40.354)	Prec@5 69.531 (66.156)
Epoch: [10][2400/2503]	Time 12.998 (12.837)	Data 4.554 (4.415)	Loss 2.8075 (2.6954)	Prec@1 37.500 (40.372)	Prec@5 64.062 (66.172)
Epoch: [10][2500/2503]	Time 12.953 (12.844)	Data 4.637 (4.422)	Loss 2.7550 (2.6946)	Prec@1 38.867 (40.376)	Prec@5 64.844 (66.181)
Test: [0/98]	Time 5.616 (5.616)	Loss 2.6384 (2.6384)	Prec@1 44.727 (44.727)	Prec@5 69.336 (69.336)
 * Prec@1 39.962 Prec@5 65.272
Learning rate: 0.001
Epoch: [11][0/2503]	Time 10.358 (10.358)	Data 1.852 (1.852)	Loss 2.7921 (2.7921)	Prec@1 35.547 (35.547)	Prec@5 65.430 (65.430)
Epoch: [11][100/2503]	Time 13.205 (13.005)	Data 4.657 (4.571)	Loss 2.7869 (2.6772)	Prec@1 38.477 (40.789)	Prec@5 62.891 (66.497)
Epoch: [11][200/2503]	Time 13.100 (13.052)	Data 4.614 (4.595)	Loss 2.6194 (2.6743)	Prec@1 38.867 (40.778)	Prec@5 66.016 (66.573)
Epoch: [11][300/2503]	Time 12.957 (13.033)	Data 4.628 (4.604)	Loss 2.7102 (2.6806)	Prec@1 40.234 (40.683)	Prec@5 67.773 (66.412)
Epoch: [11][400/2503]	Time 13.079 (13.039)	Data 4.666 (4.611)	Loss 2.4996 (2.6743)	Prec@1 42.578 (40.808)	Prec@5 71.094 (66.520)
Epoch: [11][500/2503]	Time 13.196 (13.053)	Data 4.644 (4.623)	Loss 2.6000 (2.6750)	Prec@1 42.578 (40.804)	Prec@5 68.164 (66.538)
Epoch: [11][600/2503]	Time 13.185 (13.068)	Data 4.700 (4.632)	Loss 2.7320 (2.6721)	Prec@1 41.016 (40.836)	Prec@5 66.211 (66.574)
Epoch: [11][700/2503]	Time 13.091 (13.072)	Data 4.680 (4.640)	Loss 2.6625 (2.6706)	Prec@1 40.234 (40.835)	Prec@5 70.117 (66.604)
Epoch: [11][800/2503]	Time 13.396 (13.071)	Data 4.695 (4.649)	Loss 2.5971 (2.6691)	Prec@1 41.211 (40.882)	Prec@5 67.578 (66.623)
Epoch: [11][900/2503]	Time 13.025 (13.070)	Data 4.711 (4.656)	Loss 2.5900 (2.6653)	Prec@1 41.211 (40.906)	Prec@5 68.750 (66.703)
Epoch: [11][1000/2503]	Time 13.345 (13.081)	Data 4.763 (4.663)	Loss 2.4785 (2.6635)	Prec@1 43.750 (40.944)	Prec@5 69.141 (66.753)
Epoch: [11][1100/2503]	Time 13.131 (13.092)	Data 4.748 (4.671)	Loss 2.7333 (2.6619)	Prec@1 41.211 (40.966)	Prec@5 66.406 (66.777)
Epoch: [11][1200/2503]	Time 13.502 (13.106)	Data 4.799 (4.679)	Loss 2.6643 (2.6607)	Prec@1 39.258 (40.997)	Prec@5 66.797 (66.796)
Epoch: [11][1300/2503]	Time 13.008 (13.109)	Data 4.766 (4.686)	Loss 2.7306 (2.6598)	Prec@1 41.016 (41.035)	Prec@5 65.039 (66.821)
Epoch: [11][1400/2503]	Time 13.250 (13.113)	Data 4.789 (4.693)	Loss 2.5813 (2.6588)	Prec@1 45.312 (41.057)	Prec@5 69.922 (66.845)
Epoch: [11][1500/2503]	Time 13.066 (13.121)	Data 4.805 (4.700)	Loss 2.6582 (2.6576)	Prec@1 38.281 (41.074)	Prec@5 67.969 (66.873)
Epoch: [11][1600/2503]	Time 13.155 (13.128)	Data 4.807 (4.707)	Loss 2.6104 (2.6569)	Prec@1 41.016 (41.082)	Prec@5 67.773 (66.877)
Epoch: [11][1700/2503]	Time 13.158 (13.136)	Data 4.810 (4.715)	Loss 2.6946 (2.6572)	Prec@1 41.016 (41.063)	Prec@5 64.844 (66.878)
Epoch: [11][1800/2503]	Time 13.224 (13.141)	Data 4.884 (4.722)	Loss 2.6300 (2.6559)	Prec@1 42.578 (41.078)	Prec@5 67.578 (66.902)
Epoch: [11][1900/2503]	Time 13.603 (13.151)	Data 4.903 (4.729)	Loss 2.6550 (2.6554)	Prec@1 40.234 (41.075)	Prec@5 67.578 (66.906)
Epoch: [11][2000/2503]	Time 13.693 (13.163)	Data 4.882 (4.736)	Loss 2.5359 (2.6547)	Prec@1 40.625 (41.073)	Prec@5 67.773 (66.910)
Epoch: [11][2100/2503]	Time 13.187 (13.174)	Data 4.908 (4.744)	Loss 2.4864 (2.6544)	Prec@1 46.875 (41.084)	Prec@5 70.898 (66.911)
Epoch: [11][2200/2503]	Time 13.289 (13.178)	Data 4.902 (4.751)	Loss 2.6126 (2.6546)	Prec@1 42.383 (41.088)	Prec@5 67.188 (66.901)
Epoch: [11][2300/2503]	Time 13.443 (13.185)	Data 4.918 (4.758)	Loss 2.5417 (2.6543)	Prec@1 43.945 (41.090)	Prec@5 69.141 (66.911)
Epoch: [11][2400/2503]	Time 13.371 (13.195)	Data 4.936 (4.766)	Loss 2.7484 (2.6533)	Prec@1 37.109 (41.102)	Prec@5 65.039 (66.938)
Epoch: [11][2500/2503]	Time 13.611 (13.205)	Data 5.005 (4.774)	Loss 2.6769 (2.6522)	Prec@1 37.500 (41.112)	Prec@5 66.016 (66.950)
Test: [0/98]	Time 5.646 (5.646)	Loss 2.6065 (2.6065)	Prec@1 43.164 (43.164)	Prec@5 67.578 (67.578)
 * Prec@1 40.914 Prec@5 66.508

